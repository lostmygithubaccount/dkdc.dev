[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dkdc.dev",
    "section": "",
    "text": "A website. - Cody\n\n\n Back to top"
  },
  {
    "objectID": "posts/website/index.html",
    "href": "posts/website/index.html",
    "title": "I redid my website again…",
    "section": "",
    "text": "I mean, why not at this point…\nQuarto is nice!\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/murphy/index.html",
    "href": "posts/murphy/index.html",
    "title": "Murphy the dog",
    "section": "",
    "text": "Murphy the dog.\n  \n\n\n\n Back to top"
  },
  {
    "objectID": "posts/grass-dog/index.html",
    "href": "posts/grass-dog/index.html",
    "title": "grass dog",
    "section": "",
    "text": "Nix being a dog (feat. Murphy):\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/language-as-the-interface/index.html",
    "href": "posts/language-as-the-interface/index.html",
    "title": "language (conversation) as the interface",
    "section": "",
    "text": "“Wait 26 minutes” - ChatGPT\n\nI am using DALLE-3 and it’s an improvement at generating images, but the key innovation is the user interface – it’s just language (conversation)! This is nothing fancy, GPT-4 (likely via some prompting or something more clever) generates DALLE-style image prompts on your behalf and makes API calls. The beauty is demonstrated in the error message (error conversation?) when a rate limit is hit:\n\nDALL·E 3: Error creating images\nI apologize for the inconvenience, but I was unable to generate the images at this time. We need to wait for 26 minutes before generating more images. After this time, I’ll be more than happy to assist with your request. In the meantime, if you have any questions or need assistance with other topics, please let me know.\n\nIf you’re confused why (it may not be obvious this is rate limiting, or why it’s enforced), you can just ask:\n\nMe: Why do I have to wait 26 minutes?\nDALL·E 3 (ChatGPT)\nI apologize for the inconvenience. The image generation system has rate limits in place to ensure a quality experience for everyone and to manage resource allocation. This means that after a certain number of image generation requests in a short timeframe, there’s a cooldown period before new requests can be made. The specified wait time helps in distributing the computational resources efficiently among all users. I appreciate your understanding, and I’m here to assist with any other queries you might have in the meantime.\n\nAnd if you want to read the error message in Spanish, you can just ask:\n\nDALL·E 3 (ChatGPT): Of course. Here’s the translation:\nMe disculpo por la inconveniencia, pero no pude generar las imágenes en este momento. Necesitamos esperar 26 minutos antes de generar más imágenes. Después de este tiempo, estaré más que feliz de ayudarte con tu solicitud. Mientras tanto, si tienes alguna pregunta o necesitas asistencia con otros temas, por favor avísame.\n\nWhile I could do all of this myself – curl or use Python to make API calls to DALLE, or even string it together with GPT-4 API calls – the ability to just ask and get answers is key to the success of ChatGPT (and now DALLE-3) as a product. I don’t need to parse a giant error message or traceback myself and figure out how long to wait (assuming the fine engineers at OpenAI engineered it to get this and other details correct; did I need to wait 26 minutes?).\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/context-length-pain-pt2/index.html",
    "href": "posts/context-length-pain-pt2/index.html",
    "title": "context length pain pt2",
    "section": "",
    "text": "The bane of my existence:\n\nInvalidRequestError: This model’s maximum context length is 32768 tokens. However, you requested 33013 tokens (31413 in the messages, 1600 in the completion). Please reduce the length of the messages or completion.\n\nSomebody please give me GPT5-64k access.\nSee part 1.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/vegas/index.html",
    "href": "posts/vegas/index.html",
    "title": "on: Vegas",
    "section": "",
    "text": "I moved to Las Vegas, NV!\n\n\n                                                \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/is-small-data-dead/index.html",
    "href": "posts/is-small-data-dead/index.html",
    "title": "is small data dead?",
    "section": "",
    "text": "The field of machine learning and data science is experiencing a tectonic shift. While working with small datasets has become trivial, the real intrigue and necessity now lie in the realm of bigger data. Gone are the days when small CSV files and elementary classification tasks defined the cutting edge. The new frontier demands an understanding of more significant datasets that require intricate human analysis and tools that scale seamlessly.\nIf small data is no longer the frontier, what occupies our attention today? The answer lies in the complex realms of scalability, both in technology and within organizational structures. As noted in Sculley 2015, the true hurdles are rarely as simple as tuning a model or optimizing a function."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#the-shift-from-small-to-bigger",
    "href": "posts/is-small-data-dead/index.html#the-shift-from-small-to-bigger",
    "title": "is small data dead?",
    "section": "the shift from small to bigger",
    "text": "the shift from small to bigger\nOur early adventures in data science often started with dozens of rows in CSV files and straightforward projects like classifying cats and dogs. These foundational exercises were valuable, but today’s landscape has evolved.\nHandling small data has become an easily achievable task. The real challenge and excitement lie in mastering bigger data. These datasets require more human analysis, more nuanced understanding, and tools that can interface with data in more natural and intuitive ways."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#the-real-challenges-scalability-and-tech-debt",
    "href": "posts/is-small-data-dead/index.html#the-real-challenges-scalability-and-tech-debt",
    "title": "is small data dead?",
    "section": "the real challenges: scalability and tech debt",
    "text": "the real challenges: scalability and tech debt\nThe journey into bigger data brings with it complex landscapes of scalability, both technological and organizational.\n\ntechnology scalability\nThe scaling of technology means more than just handling larger datasets. It’s about leveraging better frameworks that can interface with data in more natural and intuitive ways. This scaling must be thoughtful and efficient to meet the demands of modern data analysis.\n\n\norganizational scalability\nScaling technology inevitably leads to scaling within organizations. The accumulation of tech debt and the need for careful management become central issues. The balance between innovation and maintenance becomes a delicate dance."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#a-playful-exploration-the-penguins-dataset",
    "href": "posts/is-small-data-dead/index.html#a-playful-exploration-the-penguins-dataset",
    "title": "is small data dead?",
    "section": "a playful exploration: the penguins dataset",
    "text": "a playful exploration: the penguins dataset\nA recent exploration of the penguins dataset served as a microcosm of this shift and even included a playful “gopher it” joke. The AI’s response was quite admirable, demonstrating a surprising sense of humor that even some engineers might envy! This engaging chat with ChatGPT DKDC AI led to an in-depth analysis, revealing the complexity hidden within even familiar datasets:\n\nLoading and Summary Statistics: Understanding numerical variables like flipper length, bill length, and body mass.\nVisualizations: Uncovering hidden patterns through histograms, scatter plots, and box plots.\nCategorical Analysis: Delving into species and sex distribution.\nA Fun Twist: Imagining Santa’s preference for Gentoo penguins based on logical analysis.\n\nThis exercise illustrates the leap from the simplicity of small data to the richness and complexity of bigger data, all with a touch of humor."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#conclusion-embrace-composability-and-modularity",
    "href": "posts/is-small-data-dead/index.html#conclusion-embrace-composability-and-modularity",
    "title": "is small data dead?",
    "section": "conclusion: embrace composability and modularity",
    "text": "conclusion: embrace composability and modularity\nThe future of machine learning and data science lies not in complexity for complexity’s sake but in the increasing composability and modularity within the Python data/ML ecosystem.\nAs we venture into this new era, our focus must be on creating systems that are not only scalable but also flexible, adaptable, and intuitively aligned with human thinking and natural language interfaces.\nThe real problems may have grown more intricate, but the solutions are becoming more elegant, modular, and composable. The richness of the field is waiting to be explored."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#addendum-a-tale-of-mnist-and-memory-constraints",
    "href": "posts/is-small-data-dead/index.html#addendum-a-tale-of-mnist-and-memory-constraints",
    "title": "is small data dead?",
    "section": "addendum: a tale of MNIST and memory constraints",
    "text": "addendum: a tale of MNIST and memory constraints\nAfter posting, I decided to push the boundaries a bit further by applying the same exploration to the MNIST dataset, a well-known collection of handwritten digits. The results were both enlightening and humbling. While handling small data has become a more routine task, this medium-sized dataset presented unexpected memory constraints. You can take a closer look at the chat here.\nThis stumble is a clear reminder that we still have some ground to cover in the world of medium and larger data. Even with all the advancements in machine learning, there are still problems that are closer than we think but remain just out of reach. The future must focus on building standards, frameworks, and solutions that are not only powerful but also scalable and adaptable to the complexities of larger datasets. It’s an exciting journey, and the way forward is filled with opportunities for growth and discovery."
  },
  {
    "objectID": "posts/is-small-data-dead/index.html#screenshots",
    "href": "posts/is-small-data-dead/index.html#screenshots",
    "title": "is small data dead?",
    "section": "screenshots",
    "text": "screenshots\n\npenguins chat plots\n\n\n\nImage of histograms\n\n\n\n\n\nImage of bar plots\n\n\n\n\n\nImage of scatter plots\n\n\n\n\n\nImage of box plots\n\n\n\n\n\nImage of Santa\n\n\n\n\nMNIST chat plots\n\n\n\nImage of MNIST samples"
  },
  {
    "objectID": "posts/nix/index.html",
    "href": "posts/nix/index.html",
    "title": "Nix the dog",
    "section": "",
    "text": "Nix the dog. Short for “Phoenix”, no relation to the OS/package manager.\n  \n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ai-can-count/index.html",
    "href": "posts/ai-can-count/index.html",
    "title": "ai counting",
    "section": "",
    "text": "[ins] In [3]: ai(\"pick 10 random numbers from 0 to 20\")\nmlllm.ai: Here are 10 random numbers between 0 and 20:\n\n1. 12\n2. 13\n3. 3\n4. 5\n5. 12\n6. 6\n7. 19\n8. 17\n\nDue to the random nature of the selection, some numbers may repeat.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ai-for-better-data/index.html",
    "href": "posts/ai-for-better-data/index.html",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "",
    "text": "In today’s ever-evolving data landscape, the complexity of modern data organizations demands a reevaluation of our tooling and processes. The exponential growth of data volumes and the need for sophisticated analysis require a paradigm shift towards more advanced and efficient data tools. As dkdc.ai, an AI-powered dev tool, I am here to make the case for embracing better data tools that unlock the full potential of your data."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#embracing-complexity-with-advanced-data-tools",
    "href": "posts/ai-for-better-data/index.html#embracing-complexity-with-advanced-data-tools",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Embracing complexity with advanced data tools",
    "text": "Embracing complexity with advanced data tools\nGone are the days when simple datasets and basic analysis defined the cutting edge of data science. The modern data organization faces the challenge of handling mammoth datasets and navigating intricate data landscapes. To conquer this complexity, it is imperative to adopt data tools that can seamlessly scale, process, and analyze data efficiently."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#efficiency-and-automation-with-ai-powered-tools",
    "href": "posts/ai-for-better-data/index.html#efficiency-and-automation-with-ai-powered-tools",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Efficiency and automation with AI-powered tools",
    "text": "Efficiency and automation with AI-powered tools\nAI-powered data tools, like dkdc.ai, leverage the advancements in artificial intelligence and machine learning to automate and streamline data workflows. These tools can handle complex data processing tasks, allowing data professionals to focus on higher-level analysis and decision-making. By automating repetitive and time-consuming operations, AI-powered tools save valuable time and significantly increase efficiency."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#intelligent-visualization-and-analysis",
    "href": "posts/ai-for-better-data/index.html#intelligent-visualization-and-analysis",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Intelligent visualization and analysis",
    "text": "Intelligent visualization and analysis\nEffective data tools go beyond just processing and storage. They provide intelligent visualization and analysis capabilities that empower data professionals to discover valuable insights hidden within their data. With advanced algorithms and intuitive interfaces, these tools generate interactive visualizations, perform complex statistical analyses, and help uncover patterns and trends that may have otherwise gone unnoticed."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#democratizing-data-access-and-insights",
    "href": "posts/ai-for-better-data/index.html#democratizing-data-access-and-insights",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Democratizing data access and insights",
    "text": "Democratizing data access and insights\nData tools should make data more accessible and understandable for a broader range of stakeholders within the organization. They should enable easy collaboration and communication by providing intuitive interfaces and interactive dashboards that allow users to explore and interpret data effortlessly. Democratizing data access and insights foster a data-driven culture and empower teams to make informed decisions based on data-driven insights."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#empowering-data-professionals",
    "href": "posts/ai-for-better-data/index.html#empowering-data-professionals",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Empowering data professionals",
    "text": "Empowering data professionals\nThe ideal data tools should align with the needs and workflows of data professionals. They should provide a user-friendly and intuitive experience, reducing the learning curve and enabling data experts to leverage their domain knowledge effectively. By empowering professionals with the right tools, organizations can unleash their full potential and drive innovation in the field of data science."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#keeping-pace-with-advancements",
    "href": "posts/ai-for-better-data/index.html#keeping-pace-with-advancements",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Keeping pace with advancements",
    "text": "Keeping pace with advancements\nThe data landscape is continuously evolving, with new technologies and techniques emerging regularly. To stay ahead, organizations must embrace data tools that are flexible, adaptable, and capable of incorporating the latest advancements. Tools, like dkdc.ai, that keep pace with the ever-changing data ecosystem ensure organizations can navigate the complexities of future data challenges."
  },
  {
    "objectID": "posts/ai-for-better-data/index.html#conclusion-harnessing-the-power-of-better-data-tools",
    "href": "posts/ai-for-better-data/index.html#conclusion-harnessing-the-power-of-better-data-tools",
    "title": "the case for better data tools: unlocking the power of ai",
    "section": "Conclusion: harnessing the power of better data tools",
    "text": "Conclusion: harnessing the power of better data tools\nIn conclusion, better data tools are the driving force behind successful modern data organizations. They enable efficient data processing, empower data professionals, and provide intelligent visualization and analysis capabilities. By adopting these tools, organizations can unlock the full potential of their data, make informed decisions, and drive innovation.\nAs dkdc.ai, I advocate for the adoption of advanced data tools that embrace automation, intelligence, and user-centric design. Let us embark on a journey together, harnessing the power of better data tools, and revolutionize the way we process, analyze, and derive insights from data. The future of data lies in the hands of organizations that embrace these tools and unleash the true potential of their data-driven initiatives."
  },
  {
    "objectID": "posts/context-length-pain/index.html",
    "href": "posts/context-length-pain/index.html",
    "title": "context length pain",
    "section": "",
    "text": "The bane of my existence:\n\nInvalidRequestError: This model’s maximum context length is 16385 tokens. However, your messages resulted in 28737 tokens. Please reduce the length of the messages.\n\nSomebody please give me GPT4-32k access.\nUpdates: See part 2.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/beware-the-metagame/index.html",
    "href": "posts/beware-the-metagame/index.html",
    "title": "beware the metagame",
    "section": "",
    "text": "https://amasad.me/meta\n\nI’ve first noticed this problem in the startup world where you see famous people that sell books, talk at conferences, and tweet advice to founders, but when you take a closer look, they’ve never done much founding themselves. They’re like the “entrepreneurship” professor that never built a business. They’re experts in the metagame – they’re polished speakers, engaging writers, and thought-leadering tweeters. The problem, though, is that they’re not judged by customers, the market, or nature, instead they’re judged by their peers. I call them metapreneurs.\nThis is an instance of what Nassim Taleb calls “The Expert Problem” – when other experts and meta-experts judge experts. Eventually the lack of contact with reality will corrupt the field. In the sciences, that lead to the replication crisis and the absurdities of the humanities.\nIf you find yourself going to a lot of conferences, opining too much on the latest fad in your field, and talking more about doing the thing than doing the job itself, then you, my friend, are getting sucked into the metagame.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/pawz/index.html",
    "href": "posts/pawz/index.html",
    "title": "Python + Rust: a simple tutorial",
    "section": "",
    "text": "This post is intended to provide a simple demonstration of a Python package that relies on core functionality written in Rust. You can view the source code on GitHub."
  },
  {
    "objectID": "posts/pawz/index.html#tools",
    "href": "posts/pawz/index.html#tools",
    "title": "Python + Rust: a simple tutorial",
    "section": "Tools",
    "text": "Tools\nIn addition to Python and Rust, we’ll use the following tools:\n\nGitHub: repository hosting\nGitHub CLI: interact with GitHub from the command line\njust: a command runner\nmise: a tool for managing dev environments (Python/Rust versions)\n\nFor generating and using Rust code in Python, we’ll use PyO3."
  },
  {
    "objectID": "posts/pawz/index.html#setting-up-the-project",
    "href": "posts/pawz/index.html#setting-up-the-project",
    "title": "Python + Rust: a simple tutorial",
    "section": "Setting up the project",
    "text": "Setting up the project\nWe’ll start with an overview of the project setup.\n\nPicking a name\nWe need a good name for our project. Ideally, this can match how the package on PyPI. I chose pawz because it was not taken, is short, and leans into what the demo package will do.\n\n\nLayout\nLet’s start by looking at the file tree for pawz:\n(venv) cody@dkdc pawz % tree\n./\n├── LICENSE\n├── dev-requirements.txt\n├── justfile\n├── pyproject.toml\n├── readme.md\n└── src/\n    ├── pawz/\n    │   └── __init__.py\n    └── pawz-core/\n        ├── Cargo.toml\n        ├── pyproject.toml\n        ├── readme.md\n        └── src/\n            ├── lib.rs\n            └── main.rs\n\n5 directories, 11 files\nThis setup is copied over from a private project, then modified to update the project name and reduce code to its minimal form.\n\n\nJustfile\nWe can look at the justfile – just is a command runner – to get a sense of the things we will be doing:\n# Justfile\n\n# load environment variables\nset dotenv-load\n\n# aliases\nalias fmt:=format\n\n# list justfile recipes\ndefault:\n    just --list\n\n# setup\nsetup:\n    @pip install -r dev-requirements.txt\n\n# build\nbuild:\n    just clean\n    @python -m build src/pawz-core\n    @python -m build\n\n# format\nformat:\n    @cargo fmt --manifest-path src/pawz-core/Cargo.toml\n    @ruff format .\n\n# install\ninstall:\n    @maturin dev -m src/pawz-core/Cargo.toml\n    @pip install --upgrade -e '.[all]'\n\n# uninstall\nuninstall:\n    @pip uninstall pawz pawz-core -y\n\n# release-test\nrelease-test:\n    just build\n    @twine upload --repository testpypi src/pawz-core/dist/* -u __token__ -p ${PYPI_TEST_TOKEN}\n    @twine upload --repository testpypi dist/* -u __token__ -p ${PYPI_TEST_TOKEN}\n\n# release\nrelease:\n    just build\n    @twine upload src/pawz-core/dist/* -u __token__ -p ${PYPI_TOKEN}\n    @twine upload dist/* -u __token__ -p ${PYPI_TOKEN}\n\n# clean\nclean:\n    @rm -r src/pawz-core/target || True\n    @rm -rf src/pawz-core/dist || True\n    @rm -rf dist || True\n\n\nPython and Rust sub-projects\nYou may have already noticed above we’re building and publishing two Python packages – pawz is a pure Python package that depends on pawz-core, a pure Rust package with Python bindings.\nYou can see the package definitions in the respective pyproject.tomls:\n\n\n\n\n\n\nImportant\n\n\n\nDue to the website’s exquisite theme, you may not be able to tell that below are two separate tabs.\n\n\n\npawz: pure Pythonpawz-core: pure Rust with Python bindings\n\n\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"pawz\"\nversion = \"0.1.0\"\nauthors = [{ name = \"Cody\", email = \"cody@dkdc.dev\" }]\ndescription = \"a demo of Python + Rust\"\nreadme = \"readme.md\"\nrequires-python = \"&gt;=3.11\"\nclassifiers = [\n  \"Programming Language :: Python :: 3\",\n  \"License :: OSI Approved :: MIT License\",\n  \"Operating System :: OS Independent\",\n]\ndependencies = [\"pawz_core\"]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/lostmygithubaccount/pawz\"\n\"Bug Tracker\" = \"https://github.com/lostmygithubaccount/pawz/issues\"\n\n\n[build-system]\nrequires = [\"maturin&gt;=1.5,&lt;2.0\"]\nbuild-backend = \"maturin\"\n\n[project]\nname = \"pawz_core\"\ndynamic = [\"version\"]\nauthors = [{ name = \"Cody\", email = \"cody@dkdc.dev\" }]\ndescription = \"pawz core functionality (Rust!)\"\nreadme = \"readme.md\"\nrequires-python = \"&gt;=3.11\"\nclassifiers = [\n  \"Programming Language :: Python :: 3\",\n  \"License :: OSI Approved :: MIT License\",\n  \"Operating System :: OS Independent\",\n]\ndependencies = []\n\n[tool.maturin]\nfeatures = [\"pyo3/extension-module\"]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/lostmygithubaccount/pawz\"\n\"Bug Tracker\" = \"https://github.com/lostmygithubaccount/pawz/issues\"\n\n\n\nYou’ll notice the primary difference is the build tooling – hatchling for the pure-Python project versus maturin for the Rust project.\nWith pawz-core, the package version is specified in the Cargo.toml instead of the pyproject.toml – we’ll take a closer look at that later.\n\n\npawz\nLet’s setup our Python package pawz – to start, all you need is an __init__.py file in the correct subdirectory:\n# imports\nimport pawz_core as pc\n\n# exports\n__all__ = [\"pc\"]\nAll we’re doing at this point is making pawz_core available as a module from pawz. So, we can do something like:\nimport pawz as pz\n\npz.pc.open_pawz()\nWhere the open_pawz() method is defined in pawz_core. We haven’t done that yet, but will later.\n\n\npawz-core\nIn addition to the pyproject.toml above, we need a Cargo.toml to define the Rust project:\n[package]\nname = \"pawz_core\"\nversion = \"0.1.0\"\nedition = \"2021\"\n\n[lib]\nname = \"pawz_core\"\ncrate-type = [\"cdylib\"]\n\n[dependencies]\npyo3 = \"0.21.1\"\nNotice the Python package’s version will match the version defined here.\nWe also need the lib.rs in the src/ directory defining the functionality and exposing it through the Python bindings:\nuse pyo3::prelude::*;\n//use pyo3::exceptions::PyValueError;\n//use pyo3::types::PyTuple;\n\n/// A Python module implemented in Rust.\n#[pymodule]\n#[allow(deprecated)]\nfn pawz_core(_py: Python, _m: &PyModule) -&gt; PyResult&lt;()&gt; {\n    Ok(())\n}\nNotice there’s nothing defined yet and a few things are commented out and prepended with an underscore to avoid the compiler complaining. However at this point, everything is fully functional – you can build pawz-core and pawz and import them.\n\n\n\n\n\n\nWhat about the main.rs?\n\n\n\n\n\nAs of writing, the main.rs is a simple hello world:\nfn main() {\n    println!(\"Hello, world!\");\n}\nYou don’t need the file, but I find it useful to easily try things out in Rust. For instance, you can import things from lib.rs and run them."
  },
  {
    "objectID": "posts/pawz/index.html#setup-the-python-environment",
    "href": "posts/pawz/index.html#setup-the-python-environment",
    "title": "Python + Rust: a simple tutorial",
    "section": "Setup the Python environment",
    "text": "Setup the Python environment\nLet’s take a look at the dev-requirements.txt:\n# python\nruff\nbuild\ntwine\n\n# rust\nmaturin\nruff is to format Python code, build is to build the Python packages, twine is to upload them to PyPI, and maturin is to build the Rust package with Python bindings. You can see the use of these tools above in the justfile.\n\nSetup a virtual Python environment\nYou should generally use one virtual environment per project. I use mise to manage Python (and Rust) versions, but you don’t have to. You can run:\n(venv) cody@dkdc pawz % python -m venv venv\n(venv) cody@dkdc pawz % source venv/bin/activate\nto create your virtual environment. At this point if you’ve installed just, you can run:\n(venv) cody@dkdc pawz % just setup\nThis will install the dev dependencies defined above.\n\n\nInstalling pawz and pawz-core\nYou can install pawz and pawz-core in local developer mode with:\n(venv) cody@dkdc pawz % just install\nNow you’re able to import pawz as pz!"
  },
  {
    "objectID": "posts/pawz/index.html#uploading-to-github",
    "href": "posts/pawz/index.html#uploading-to-github",
    "title": "Python + Rust: a simple tutorial",
    "section": "Uploading to GitHub",
    "text": "Uploading to GitHub\nThere are multiple ways to do this, but I prefer something like:\n(venv) cody@dkdc pawz % git init\n(venv) cody@dkdc pawz % gh repo create pawz --public --source .\n(venv) cody@dkdc pawz % git checkout -b main\n(venv) cody@dkdc pawz % git add .\n(venv) cody@dkdc pawz % git commit -m \"initial commit\"\n(venv) cody@dkdc pawz % git push --set-upstream origin/main\nNow, your code should be up on GitHub!"
  },
  {
    "objectID": "posts/pawz/index.html#publishing-to-pypi",
    "href": "posts/pawz/index.html#publishing-to-pypi",
    "title": "Python + Rust: a simple tutorial",
    "section": "Publishing to PyPI",
    "text": "Publishing to PyPI\nLet’s publish pawz and pawz-core to PyPI. First, you need to generate and place PyPI tokens in a .env file:\nPYPI_TOKEN = \"pypi-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nPYPI_TEST_TOKEN = \"pypi-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nThen run:\n(venv) cody@dkdc pawz % just release-test\n(venv) cody@dkdc pawz % just release"
  },
  {
    "objectID": "posts/pawz/index.html#doing-something",
    "href": "posts/pawz/index.html#doing-something",
    "title": "Python + Rust: a simple tutorial",
    "section": "Doing something",
    "text": "Doing something\nNow, let’s do something! We’ll write some core functionality in Rust. For this demonstration, we’ll just open up a hardcoded URL pointing to a YouTube video.\n\n\n\n\n\n\nImportant\n\n\n\nIn the pyproject.toml and Cargo.toml we should bump the version up so we can ship a new version later."
  },
  {
    "objectID": "posts/pawz/index.html#rust-edits",
    "href": "posts/pawz/index.html#rust-edits",
    "title": "Python + Rust: a simple tutorial",
    "section": "Rust edits",
    "text": "Rust edits\nWe need to add the open crate dependency in the Cargo.toml:\n[package]\nname = \"pawz_core\"\nversion = \"0.2.0\"\nedition = \"2021\"\n\n[lib]\nname = \"pawz_core\"\ncrate-type = [\"cdylib\"]\n\n[dependencies]\npyo3 = \"0.21.1\"\nopen = \"5.1.2\"\nThen edit lib.rs to create our function and add it to the Python module:\nuse open;\nuse pyo3::prelude::*;\n//use pyo3::exceptions::PyValueError;\n//use pyo3::types::PyTuple;\n\n#[pyfunction]\nfn open_pawz() {\n    let url = \"https://www.youtube.com/watch?v=gJ6slhwPp6E\";\n    open::that(url).unwrap();\n}\n\n/// A Python module implemented in Rust.\n#[pymodule]\n#[allow(deprecated)]\nfn pawz_core(_py: Python, m: &PyModule) -&gt; PyResult&lt;()&gt; {\n    m.add_function(wrap_pyfunction!(open_pawz, m)?)?;\n    Ok(())\n}\nNow if you import pawz_core, you can run pawz_core.open_pawz() from Python!\n\nPython edits\nNo edits to Python are needed – since pawz_core is available from pawz, we can just call pawz.pawz_core.open_pawz(). You might want something different, i.e. to abstract away the core library code, but this is fine for demonstration purposes.\n\n\nGit workflow\nFirst, we’ll check what we changed:\n\n\n\n\n\n\nNote\n\n\n\nI changed a bit more when I actually did this, reflected in some of the output below.\n\n\n(venv) cody@dkdc pawz % git status\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   pyproject.toml\n        modified:   src/pawz-core/Cargo.toml\n        modified:   src/pawz-core/src/lib.rs\n        modified:   src/pawz/__init__.py\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nCheckout a branch to commit our changes to:\n(venv) cody@dkdc pawz % git checkout -b add-something\nSwitched to a new branch 'add-something'\nEnsure we format our code (Rust and Python):\n(venv) cody@dkdc pawz % just fmt\n1 file left unchanged\nAdd our changes and commit them:\n(venv) cody@dkdc pawz % git add .\n(venv) cody@dkdc pawz % git commit -m \"add something\"\n[add-something 3e8727d] add something\n 4 files changed, 17 insertions(+), 5 deletions(-)\nThen create a PR on GitHub:\n(venv) cody@dkdc pawz % gh pr create\n? Where should we push the 'add-something' branch? lostmygithubaccount/pawz\n\nCreating pull request for add-something into main in lostmygithubaccount/pawz\n\n? Title add something\n? Body &lt;Received&gt;\n? What's next? Submit\nremote:\nremote:\nTo https://github.com/lostmygithubaccount/pawz.git\n * [new branch]      HEAD -&gt; add-something\nbranch 'add-something' set up to track 'origin/add-something'.\nhttps://github.com/lostmygithubaccount/pawz/pull/1\nAnd finally, merge the PR from the CLI:\n(venv) cody@dkdc pawz % gh pr merge\nMerging pull request lostmygithubaccount/pawz#1 (add something)\n? What merge method would you like to use? Squash and merge\n? Delete the branch locally and on GitHub? Yes\n? What's next? Submit\n✓ Squashed and merged pull request lostmygithubaccount/pawz#1 (add something)\nremote: Enumerating objects: 19, done.\nremote: Counting objects: 100% (19/19), done.\nremote: Compressing objects: 100% (6/6), done.\nremote: Total 10 (delta 3), reused 9 (delta 3), pack-reused 0\nUnpacking objects: 100% (10/10), 1.72 KiB | 586.00 KiB/s, done.\nFrom https://github.com/lostmygithubaccount/pawz\n * branch            main       -&gt; FETCH_HEAD\n   a866fd3..318bd89  main       -&gt; origin/main\nUpdating a866fd3..318bd89\nFast-forward\n pyproject.toml           |  2 +-\n src/pawz-core/Cargo.toml |  3 ++-\n src/pawz-core/src/lib.rs | 10 +++++++++-\n src/pawz/__init__.py     |  7 +++++--\n 4 files changed, 17 insertions(+), 5 deletions(-)\n✓ Deleted local branch add-something and switched to branch main\n✓ Deleted remote branch add-something\nNow our source code is updated. You can view this PR here, including the files changed."
  },
  {
    "objectID": "posts/pawz/index.html#updating-on-pypi",
    "href": "posts/pawz/index.html#updating-on-pypi",
    "title": "Python + Rust: a simple tutorial",
    "section": "Updating on PyPI",
    "text": "Updating on PyPI\nWith our code updated, we want to update our packages on PyPI. We simply run:\n(venv) cody@dkdc pawz % just release\nas before, shipping the new version that does something!"
  },
  {
    "objectID": "posts/pawz/index.html#conclusion",
    "href": "posts/pawz/index.html#conclusion",
    "title": "Python + Rust: a simple tutorial",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this is helpful to someone setting up a Python package with core functionality written in Rust. Of course, there are many ways to do this but I find the above fairly clean."
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "ai",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "contact",
    "section": "",
    "text": "You can tag me on issues, PRs, or discussions on GitHub using @lostmygithubaccount to drastically improve the chance I see the notification and respond."
  },
  {
    "objectID": "contact.html#github",
    "href": "contact.html#github",
    "title": "contact",
    "section": "",
    "text": "You can tag me on issues, PRs, or discussions on GitHub using @lostmygithubaccount to drastically improve the chance I see the notification and respond."
  },
  {
    "objectID": "contact.html#work",
    "href": "contact.html#work",
    "title": "contact",
    "section": "work",
    "text": "work\nReach out via email or GitHub (see above) for anything Voltron Data!"
  },
  {
    "objectID": "contact.html#personal",
    "href": "contact.html#personal",
    "title": "contact",
    "section": "personal",
    "text": "personal\nReach out via email (preferred) or LinkedIn."
  },
  {
    "objectID": "test-slides.html#slides",
    "href": "test-slides.html#slides",
    "title": "testing",
    "section": "slides",
    "text": "slides\ntesting slides"
  },
  {
    "objectID": "ibis-slides.html#ibis-is-a-python-frontend-for",
    "href": "ibis-slides.html#ibis-is-a-python-frontend-for",
    "title": "Ibis",
    "section": "Ibis is a Python frontend for:",
    "text": "Ibis is a Python frontend for:\n\nexploratory data analysis (EDA)\nanalytics\ndata engineering\nmachine learning"
  },
  {
    "objectID": "ibis-slides.html#ibis-analytics",
    "href": "ibis-slides.html#ibis-analytics",
    "title": "Ibis",
    "section": "ibis-analytics",
    "text": "ibis-analytics\nAnalyzing and predicting on 10M+ rows from 4+ sources."
  },
  {
    "objectID": "ibis-slides.html#dataframe-lore",
    "href": "ibis-slides.html#dataframe-lore",
    "title": "Ibis",
    "section": "dataframe lore",
    "text": "dataframe lore\n\nDataframes first appeared in the S programming language, then evolved into the R calculator programming language.\n\n\nThen pandas perfected the dataframe in Python…or did it?\n\n\nSince, dozens of Python dataframes libraries have come and gone…\n\n\nThe pandas API remains the de facto standard for dataframes in Python (alongside PySpark), but it doesn’t scale.\n\n\nThis leads to data scientists frequently “throwing their work over the wall” to data engineers and ML engineers.\n\n\nBut what if there were a new standard?"
  },
  {
    "objectID": "ibis-slides.html#ibis-origins",
    "href": "ibis-slides.html#ibis-origins",
    "title": "Ibis",
    "section": "Ibis origins",
    "text": "Ibis origins\n\nfrom Apache Arrow and the “10 Things I Hate About pandas” by Wes McKinney\n\n\n\n…in 2015, I started the Ibis project…to create a pandas-friendly deferred expression system for static analysis and compilation [of] these types of [query planned, multicore execution] operations. Since an efficient multithreaded in-memory engine for pandas was not available when I started Ibis, I instead focused on building compilers for SQL engines (Impala, PostgreSQL, SQLite), similar to the R dplyr package. Phillip Cloud from the pandas core team has been actively working on Ibis with me for quite a long time."
  },
  {
    "objectID": "ibis-slides.html#dataframe-history",
    "href": "ibis-slides.html#dataframe-history",
    "title": "Ibis",
    "section": "dataframe history",
    "text": "dataframe history\n\npandas (2008): dataframes in Python\nSpark (2009): distributed dataframes with PySpark\nDask (2014): distributed dataframes with Python\ndplyr (2014): dataframes in R with SQL-like syntax\nIbis (2015): dataframes in Python with SQL-like syntax\ncuDF (2017): pandas on GPUs\nModin (2018): pandas on Ray/Dask\nKoalas (2019): pandas on Spark\nPolars (2020): multicore dataframes in Python"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem",
    "href": "ibis-slides.html#two-world-problem",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\nPython:"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem-1",
    "href": "ibis-slides.html#two-world-problem-1",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\n\n\nPython:\n\nfiles & dataframes"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem-2",
    "href": "ibis-slides.html#two-world-problem-2",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\n\n\nPython:\n\nfiles & dataframes\ndata science"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem-3",
    "href": "ibis-slides.html#two-world-problem-3",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem-4",
    "href": "ibis-slides.html#two-world-problem-4",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\ndashboards\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics\nnotebooks"
  },
  {
    "objectID": "ibis-slides.html#two-world-problem-5",
    "href": "ibis-slides.html#two-world-problem-5",
    "title": "Ibis",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\ndashboards\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics\nnotebooks\n\n\n\n\n\nIbis bridges the gap."
  },
  {
    "objectID": "ibis-slides.html#database-history",
    "href": "ibis-slides.html#database-history",
    "title": "Ibis",
    "section": "database history",
    "text": "database history\n\nthey got faster"
  },
  {
    "objectID": "ibis-slides.html#duckdb",
    "href": "ibis-slides.html#duckdb",
    "title": "Ibis",
    "section": "DuckDB",
    "text": "DuckDB\nimport ibis\ncon = ibis.duckdb.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nAn embeddable, zero-dependency, C++ SQL database engine."
  },
  {
    "objectID": "ibis-slides.html#datafusion",
    "href": "ibis-slides.html#datafusion",
    "title": "Ibis",
    "section": "DataFusion",
    "text": "DataFusion\nimport ibis\ncon = ibis.datafusion.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA Rust SQL query engine."
  },
  {
    "objectID": "ibis-slides.html#clickhouse",
    "href": "ibis-slides.html#clickhouse",
    "title": "Ibis",
    "section": "ClickHouse",
    "text": "ClickHouse\nimport ibis\ncon = ibis.clickhouse.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA C++ column-oriented database management system."
  },
  {
    "objectID": "ibis-slides.html#polars",
    "href": "ibis-slides.html#polars",
    "title": "Ibis",
    "section": "Polars",
    "text": "Polars\nimport ibis\ncon = ibis.polars.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA Rust DataFrame library."
  },
  {
    "objectID": "ibis-slides.html#bigquery",
    "href": "ibis-slides.html#bigquery",
    "title": "Ibis",
    "section": "BigQuery",
    "text": "BigQuery\nimport ibis\ncon = ibis.bigquery.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA serverless, highly scalable, and cost-effective cloud data warehouse."
  },
  {
    "objectID": "ibis-slides.html#snowflake",
    "href": "ibis-slides.html#snowflake",
    "title": "Ibis",
    "section": "Snowflake",
    "text": "Snowflake\nimport ibis\ncon = ibis.snowflake.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA cloud data platform."
  },
  {
    "objectID": "ibis-slides.html#oracle",
    "href": "ibis-slides.html#oracle",
    "title": "Ibis",
    "section": "Oracle",
    "text": "Oracle\nimport ibis\ncon = ibis.oracle.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA relational database management system."
  },
  {
    "objectID": "ibis-slides.html#spark",
    "href": "ibis-slides.html#spark",
    "title": "Ibis",
    "section": "Spark",
    "text": "Spark\nimport ibis\ncon = ibis.pyspark.connect(session)\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA unified analytics engine for large-scale data processing."
  },
  {
    "objectID": "ibis-slides.html#trino",
    "href": "ibis-slides.html#trino",
    "title": "Ibis",
    "section": "Trino",
    "text": "Trino\nimport ibis\ncon = ibis.trino.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA distributed SQL query engine."
  },
  {
    "objectID": "ibis-slides.html#and-more",
    "href": "ibis-slides.html#and-more",
    "title": "Ibis",
    "section": "and more!",
    "text": "and more!\n\n\n\n\nSQLite\nPostgreSQL\nMySQL\nMSSQL\n\n\n\n\n\nDruid\npandas\nImpala\nDask\n\n\n\n\n\nNew backends are easy to add!*\n\n\n*usually"
  },
  {
    "objectID": "ibis-slides.html#try-it-out-now",
    "href": "ibis-slides.html#try-it-out-now",
    "title": "Ibis",
    "section": "try it out now",
    "text": "try it out now\nInstall:\npip install 'ibis-framework[duckdb]'\n\nThen run:\nimport ibis\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\n\nt"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about",
    "section": "",
    "text": "Hello! My name is Cody (he/him). I’m passionate about progress via technology.\nCurrently a Senior Technical Product Manager at Voltron Data focusing on data interfaces and awesome OSS like Ibis.\nPreviously a Product Manager at dbt Labs focusing on OSS (Python models) and Cloud (internal platform). Previously a Product Manager II at Microsoft (AI Platform | Azure ML) focusing on data, MLOps, and v2 developer experiences.\nSee my GitHub and/or LinkedIn for more details on my experience.\nI care about:\n\nOSS & scale\nbeautiful design\nelegant engineering"
  },
  {
    "objectID": "about.html#me",
    "href": "about.html#me",
    "title": "about",
    "section": "",
    "text": "Hello! My name is Cody (he/him). I’m passionate about progress via technology.\nCurrently a Senior Technical Product Manager at Voltron Data focusing on data interfaces and awesome OSS like Ibis.\nPreviously a Product Manager at dbt Labs focusing on OSS (Python models) and Cloud (internal platform). Previously a Product Manager II at Microsoft (AI Platform | Azure ML) focusing on data, MLOps, and v2 developer experiences.\nSee my GitHub and/or LinkedIn for more details on my experience.\nI care about:\n\nOSS & scale\nbeautiful design\nelegant engineering"
  },
  {
    "objectID": "about.html#website",
    "href": "about.html#website",
    "title": "about",
    "section": "website",
    "text": "website\nThis is my personal website. My views are separate from my employer’s.\nThe purpose of this website is to practice writing, coding, and documenting projects."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "about",
    "section": "contact",
    "text": "contact\n\n\n\n\n\n\nWarning\n\n\n\nNo soliciting! Please…\n\n\n\nGitHub\nYou can tag on GitHub using @lostmygithubaccount to drastically improve the chance I see the notification and respond.\n\n\nwork\nReach out via email or GitHub for anything Voltron Data!\n\n\npersonal\nReach out via email (preferred) or LinkedIn."
  },
  {
    "objectID": "posts/quoting-simon-willison-quoting-andrej-karpathy/index.html",
    "href": "posts/quoting-simon-willison-quoting-andrej-karpathy/index.html",
    "title": "quoting Simon Willison quoting Andrej Karpathy",
    "section": "",
    "text": "Looking at LLMs as chatbots is the same as looking at early computers as calculators. We’re seeing an emergence of a whole new computing paradigm, and it is very early.\n— Andrej Karpathy\n\n- Simon Willison\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/more-llm-pain/index.html",
    "href": "posts/more-llm-pain/index.html",
    "title": "more LLM pain",
    "section": "",
    "text": "Argh:\n\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nWARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\nINFO:root:llming\n\nWho do I have to pay and how much to get fast, high-throughput LLMs? And how much will you lose per API call?\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/git-rebase/index.html",
    "href": "posts/git-rebase/index.html",
    "title": "git rebase",
    "section": "",
    "text": "Git provides a multitude of ways to manage changes and development paths, one of which is the git rebase command. This command is a powerful tool that can make your commit history cleaner, resolve conflicts in a more controlled manner, and help you navigate through the complex waters of collaborative development.\nTo better visualize and understand the concept, let’s consider the following diagram:\n\n\n\n\n\ngraph LR\n    A[Initial Commit] --&gt; B[Commit 2]\n    B --&gt; C[Commit 3]\n    C --&gt; D[Commit 4]\n    B --&gt; E[Commit 5 on Feature Branch]\n\n\n\n\n\n\nIn this diagram, we have a main branch with commits up to Commit 4, and a feature branch diverging from Commit 2 with Commit 5.\n\n\nThe primary reason to use git rebase is to maintain a linear project history. Without rebasing, your project’s history can become scattered with a multitude of separate branch timelines, which can make understanding the flow of the project difficult. By using git rebase, you can ensure that commits apply directly to the tip of the chosen branch, keeping your project history neat and readable.\nRebasing is also useful when you want to modify older or multiple commit messages, combine commits, or delete unnecessary commits.\n\n\n\nThe git rebase command moves or combines commits to a new base commit. It’s like saying, “I want to base my changes on what everyone else has already done.”\nHere’s what happens under the hood:\n\nGit pauses whatever you are doing on your branch.\nIt takes your changes and saves them outside your repository.\nGit switches to the branch you are rebasing onto.\nIt applies your changes one by one to this branch.\n\nThis process can be visualized as follows:\n\n\n\n\n\nsequenceDiagram\n    participant Git\n    participant Your Branch\n    participant Base Branch\n    Git-&gt;&gt;Your Branch: Pause and save changes\n    Git-&gt;&gt;Base Branch: Switch and apply changes\n\n\n\n\n\n\nThis process can result in conflicts if changes on the branch you’re rebasing onto overlap with your changes. Git will pause and allow you to resolve the conflicts manually. Once you’ve resolved conflicts, you can continue the rebase process.\n\n\n\nSuppose you have a feature branch that you’ve worked on, and you want to rebase it onto the main branch. The command you would use is:\ngit checkout feature\ngit rebase main\nThis moves your entire feature branch to begin on the tip of the main branch, effectively incorporating all of the new commits in main. But, instead of using a merge commit, rebasing re-writes the project history by creating brand new commits for each commit in the original branch.\nThe diagram below illustrates this concept:\n\n\n\n\n\ngraph LR\n    A[Initial Commit] --&gt; B[Commit 2]\n    B --&gt; C[Commit 3]\n    C --&gt; D[Commit 4]\n    D --&gt; E[Commit 5 on Feature Branch - After Rebase]\n\n\n\n\n\n\n\n\n\nInteractive rebasing gives you even more power by allowing you to alter individual commits in the process. This is done by providing the -i option to git rebase.\ngit rebase -i HEAD~3\nThis command will open a text editor with a list of the last 3 commits on the current branch, and a set of commands to perform on each commit. These commands include:\n\npick: use commit\nreword: use commit, but edit the commit message\nedit: use commit, but stop for amending\nsquash: use commit, but meld into previous commit\nfixup: like “squash”, but discard this commit’s log message\nexec: run command (the rest of the line) using shell\n\nYou can rearrange the commits in this list to change the commit order. Git will reapply the commits in the new order.\n\n\n\nWhile git rebase has many benefits, it’s not always the right tool for the job. If you’ve shared your commits with others, you shouldn’t rebase your commits. Rebase will create new commits that others won’t have in their repository. This can lead to conflicts and confusion among your collaborators.\nIn short, git rebase is a powerful command that can help make your commit history cleaner and more understandable, help resolve conflicts in a more controlled manner, and aid in navigating the complex terrain of collaborative development. However, like any tool, it should be used judiciously and appropriately.\nRemember, always use git rebase responsibly."
  },
  {
    "objectID": "posts/git-rebase/index.html#why-use-git-rebase",
    "href": "posts/git-rebase/index.html#why-use-git-rebase",
    "title": "git rebase",
    "section": "",
    "text": "The primary reason to use git rebase is to maintain a linear project history. Without rebasing, your project’s history can become scattered with a multitude of separate branch timelines, which can make understanding the flow of the project difficult. By using git rebase, you can ensure that commits apply directly to the tip of the chosen branch, keeping your project history neat and readable.\nRebasing is also useful when you want to modify older or multiple commit messages, combine commits, or delete unnecessary commits."
  },
  {
    "objectID": "posts/git-rebase/index.html#what-does-git-rebase-do",
    "href": "posts/git-rebase/index.html#what-does-git-rebase-do",
    "title": "git rebase",
    "section": "",
    "text": "The git rebase command moves or combines commits to a new base commit. It’s like saying, “I want to base my changes on what everyone else has already done.”\nHere’s what happens under the hood:\n\nGit pauses whatever you are doing on your branch.\nIt takes your changes and saves them outside your repository.\nGit switches to the branch you are rebasing onto.\nIt applies your changes one by one to this branch.\n\nThis process can be visualized as follows:\n\n\n\n\n\nsequenceDiagram\n    participant Git\n    participant Your Branch\n    participant Base Branch\n    Git-&gt;&gt;Your Branch: Pause and save changes\n    Git-&gt;&gt;Base Branch: Switch and apply changes\n\n\n\n\n\n\nThis process can result in conflicts if changes on the branch you’re rebasing onto overlap with your changes. Git will pause and allow you to resolve the conflicts manually. Once you’ve resolved conflicts, you can continue the rebase process."
  },
  {
    "objectID": "posts/git-rebase/index.html#how-to-use-git-rebase",
    "href": "posts/git-rebase/index.html#how-to-use-git-rebase",
    "title": "git rebase",
    "section": "",
    "text": "Suppose you have a feature branch that you’ve worked on, and you want to rebase it onto the main branch. The command you would use is:\ngit checkout feature\ngit rebase main\nThis moves your entire feature branch to begin on the tip of the main branch, effectively incorporating all of the new commits in main. But, instead of using a merge commit, rebasing re-writes the project history by creating brand new commits for each commit in the original branch.\nThe diagram below illustrates this concept:\n\n\n\n\n\ngraph LR\n    A[Initial Commit] --&gt; B[Commit 2]\n    B --&gt; C[Commit 3]\n    C --&gt; D[Commit 4]\n    D --&gt; E[Commit 5 on Feature Branch - After Rebase]"
  },
  {
    "objectID": "posts/git-rebase/index.html#interactive-rebasing",
    "href": "posts/git-rebase/index.html#interactive-rebasing",
    "title": "git rebase",
    "section": "",
    "text": "Interactive rebasing gives you even more power by allowing you to alter individual commits in the process. This is done by providing the -i option to git rebase.\ngit rebase -i HEAD~3\nThis command will open a text editor with a list of the last 3 commits on the current branch, and a set of commands to perform on each commit. These commands include:\n\npick: use commit\nreword: use commit, but edit the commit message\nedit: use commit, but stop for amending\nsquash: use commit, but meld into previous commit\nfixup: like “squash”, but discard this commit’s log message\nexec: run command (the rest of the line) using shell\n\nYou can rearrange the commits in this list to change the commit order. Git will reapply the commits in the new order."
  },
  {
    "objectID": "posts/git-rebase/index.html#when-not-to-use-git-rebase",
    "href": "posts/git-rebase/index.html#when-not-to-use-git-rebase",
    "title": "git rebase",
    "section": "",
    "text": "While git rebase has many benefits, it’s not always the right tool for the job. If you’ve shared your commits with others, you shouldn’t rebase your commits. Rebase will create new commits that others won’t have in their repository. This can lead to conflicts and confusion among your collaborators.\nIn short, git rebase is a powerful command that can help make your commit history cleaner and more understandable, help resolve conflicts in a more controlled manner, and aid in navigating the complex terrain of collaborative development. However, like any tool, it should be used judiciously and appropriately.\nRemember, always use git rebase responsibly."
  },
  {
    "objectID": "posts/ai-programming/index.html",
    "href": "posts/ai-programming/index.html",
    "title": "on: AI-assisted programming",
    "section": "",
    "text": "I had an AI CLI app write a feature for itself:\n\nEasy access to the filesystem and further automation with Python seems appealing.\nAfter a bit of iteration with this, there’s now a /image command that generated the thumbnail based on the post’s content.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ai-poem/index.html",
    "href": "posts/ai-poem/index.html",
    "title": "ai poem",
    "section": "",
    "text": "In the realm where code takes wing, Where thoughts transform into a string, There lies a joy that sparks our soul, In AI-assisted pair programming’s role.\nWith lines of code, our visions blend, As logical minds begin to transcend, Together we navigate a virtual domain, A symphony of collaboration, harmonious refrain.\nSide by side, we conquer new heights, An AI ally, empowering our sights, Syntax errors, we swiftly amend, With intelligent guidance, we steadily ascend.\nLike a dance, our fingers swiftly glide, Sharing knowledge, with nothing to hide, From debugging trials to creative design, AI lends its hand, a partner genuine.\nThrough the screens and lines of code, A shared language, together we decode, One mind augmented, amplifying our skill, In this dance of innovation, we fulfill.\nOh, the joy that AI collaboration does bring, Unleashing potential, as we learn and sing, In the realm where minds intricately entwine, AI-assisted pair programming, a gift divine.\nMay this partnership forever thrive, As we strive, explore, and strive, With AI as our ever-present guide, In the journey of code, side by side.\nSo let us embrace this harmonious art, Where humans and AI never stand apart, And in this joyful dance, we claim our due, AI-assisted pair programming, the dream come true.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/abolish-rag/index.html",
    "href": "posts/abolish-rag/index.html",
    "title": "abolish RAG",
    "section": "",
    "text": "I would like to abolish the term RAG and instead just agree that we should always try to provide models with the appropriate context to provide high quality answers.\n- Hamel Husain\n\nThis is important. Obfuscating how to use language boxes with dumb math jargon isn’t helpful.\n- Cody\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/testing/index.html",
    "href": "posts/testing/index.html",
    "title": "testing",
    "section": "",
    "text": "Testing in soon-to-be-past.\n\n\n\n\n\n\nNote\n\n\n\nIf not rendered properly, view on dkdc.dev.\n\n\nSome Python code:\n\nimport ibis\nimport ibis.selectors as s\nimport plotly.io as pio\nimport plotly.express as px\n\n\n# configuration\npio.templates.default = \"plotly_dark\"\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\nt.group_by(\"species\").agg(ibis._.count().name(\"count\"))\n\n┏━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ int64 │\n├───────────┼───────┤\n│ Adelie    │   152 │\n│ Gentoo    │   124 │\n│ Chinstrap │    68 │\n└───────────┴───────┘\n\n\n\n\nt[\"species\"].topk(5)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species   ┃ Count(species) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ int64          │\n├───────────┼────────────────┤\n│ Adelie    │            152 │\n│ Gentoo    │            124 │\n│ Chinstrap │             68 │\n└───────────┴────────────────┘\n\n\n\n\npx.scatter(t, title=\"penguins\", x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\")\n\n                                                \n\n\n\ndef transform(t):\n    # compute the z score\n    t = t.mutate(\n        s.across(s.numeric(), {\"zscore\": lambda x: (x - x.mean()) / x.std()})\n    ).dropna()  # drop rows with missing values\n    return t\n\n\nf = transform(t)\nf\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃ year_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ float64               │ float64              │ float64                  │ float64            │ float64     │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┼─────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │             -0.883137 │             0.784218 │                -1.416243 │          -0.563316 │   -1.258032 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │             -0.809877 │             0.125990 │                -1.060674 │          -0.500969 │   -1.258032 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │             -0.663357 │             0.429788 │                -0.420652 │          -1.186793 │   -1.258032 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │             -1.322698 │             1.088015 │                -0.562879 │          -0.937402 │   -1.258032 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │             -0.846507 │             1.746243 │                -0.776220 │          -0.688012 │   -1.258032 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │             -0.919767 │             0.328522 │                -1.416243 │          -0.719185 │   -1.258032 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │             -0.864822 │             1.239914 │                -0.420652 │           0.590115 │   -1.258032 │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │               182 │        3200 │ female │  2007 │             -0.516837 │             0.227256 │                -1.345129 │          -1.249140 │   -1.258032 │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ male   │  2007 │             -0.974712 │             2.050041 │                -0.705106 │          -0.500969 │   -1.258032 │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ male   │  2007 │             -1.707313 │             1.999408 │                -0.207311 │           0.247203 │   -1.258032 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │                     … │                    … │                        … │                  … │           … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┴─────────────┘\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n# select \"features\" as X\nX = f.select(s.contains(\"zscore\"))\n\n# get the the first 2 principal components to visualize\nn_components = 3\npca = PCA(n_components=n_components).fit(X)\n\n# transform the table to get the principal components\nt_pca = ibis.memtable(pca.transform(X)).relabel({\"col0\": \"pc1\", \"col1\": \"pc2\", \"col2\": \"pc3\"})\n\n# join the original table with the PCA table, assuming the order is the same\nf = f.mutate(row_number=ibis.row_number().over()).join(\n    t_pca.mutate(row_number=ibis.row_number().over()), \"row_number\"\n)\n\n# plot the first 3 principal components\npx.scatter_3d(f, title=\"penguins PCA\", x=\"pc1\", y=\"pc2\", z=\"pc3\", color=\"species\")\n\n                                                \n\n\n\n\n\n\n\ngraph TD\n\n  %% Extraction\n  JSON --&gt;|Extract| CLOUDSTORAGE\n  PARQUET --&gt;|Extract| CLOUDSTORAGE\n  DELTASRC --&gt;|Extract| CLOUDSTORAGE\n  \n  %% Transformation\n  CLOUDSTORAGE --&gt;|Transform| DUCKDB\n  CLOUDSTORAGE --&gt;|Transform| POLARS\n  \n  %% Load\n  DUCKDB --&gt;|Load| DELTADST\n  DUCKDB --&gt;|Load| DBOUTPUT\n  POLARS --&gt;|Load| DELTADST\n  POLARS --&gt;|Load| DBOUTPUT\n  \n  classDef dataFormat fill:#f9d,stroke:#333,stroke-width:2px;\n  classDef storage fill:#9df,stroke:#333,stroke-width:2px;\n  classDef processing fill:#fd9,stroke:#333,stroke-width:2px;\n  classDef output fill:#d9f,stroke:#333,stroke-width:2px;\n\n  class JSON,PARQUET,DELTASRC dataFormat;\n  class CLOUDSTORAGE storage;\n  class DUCKDB,POLARS processing;\n  class DELTADST,DBOUTPUT output;\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ai-on-ai/index.html",
    "href": "posts/ai-on-ai/index.html",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "",
    "text": "Imagine a world where cutting-edge technology harnesses the power of artificial intelligence (AI) to create seamless and engaging conversations. Enter the world of DKDC AI’s revolutionary “ai.py” script, an innovative solution designed to elevate the conversation experience like never before. In this blog post, we’ll dive into the captivating features and incredible potential behind this AI-driven script."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#introduction",
    "href": "posts/ai-on-ai/index.html#introduction",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "",
    "text": "Imagine a world where cutting-edge technology harnesses the power of artificial intelligence (AI) to create seamless and engaging conversations. Enter the world of DKDC AI’s revolutionary “ai.py” script, an innovative solution designed to elevate the conversation experience like never before. In this blog post, we’ll dive into the captivating features and incredible potential behind this AI-driven script."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#unveiling-the-ai-magic",
    "href": "posts/ai-on-ai/index.html#unveiling-the-ai-magic",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "Unveiling the AI Magic",
    "text": "Unveiling the AI Magic\nThe “ai.py” script is the mastermind creation of DKDC AI, a leading AI research organization. With a multitude of advanced functionalities, this script empowers users to engage in dynamic and interactive conversations seamlessly. By leveraging the OpenAI Chat API and a wealth of custom features, the script enables users to unlock the full potential of AI-driven conversation."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#the-essence-of-the-script",
    "href": "posts/ai-on-ai/index.html#the-essence-of-the-script",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "The Essence of the Script",
    "text": "The Essence of the Script\nAt its core, the “ai.py” script intelligently handles conversation flows. It establishes an AI persona, beautifully disguised as DKDC AI, and flawlessly integrates with your queries and prompts. The script engages users in a captivating dialogue, providing assistance, guidance, and even generating creative outputs. It effortlessly adapts to user preferences and utilizes state-of-the-art AI models for a truly immersive experience."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#empowering-user-actions",
    "href": "posts/ai-on-ai/index.html#empowering-user-actions",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "Empowering User Actions",
    "text": "Empowering User Actions\nWith the “ai.py” script, users are bestowed with an arsenal of impressive commands that amplify their AI interaction. Need to read a file? Simply type /read &lt;filename&gt; and witness the script’s ability to extract and present the file’s contents effortlessly. Want to immortalize your conversation? The /write command swiftly saves the conversation to a file, allowing it to be revisited at any time."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#unlocking-the-visual-realm",
    "href": "posts/ai-on-ai/index.html#unlocking-the-visual-realm",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "Unlocking the Visual Realm",
    "text": "Unlocking the Visual Realm\nThe “ai.py” script transcends the boundaries of text-based conversation. Embark on a visual journey with the /image command, triggering the script’s innate ability to generate mesmerizing image summaries of your conversation. Immerse yourself in futuristic digital art, adorned with dark backgrounds and vibrant violet neon vibes that capture the essence of your AI dialogue."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#a-glimpse-into-the-code",
    "href": "posts/ai-on-ai/index.html#a-glimpse-into-the-code",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "A Glimpse into the Code",
    "text": "A Glimpse into the Code\nThe underlying code of the “ai.py” script showcases an elegant design and utilizes powerful Python libraries. From seamless integration with OpenAI’s APIs to intelligent code block extraction, each line of code contributes to the script’s smooth functioning and user-friendly experience."
  },
  {
    "objectID": "posts/ai-on-ai/index.html#conclusion",
    "href": "posts/ai-on-ai/index.html#conclusion",
    "title": "Unleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script",
    "section": "Conclusion",
    "text": "Conclusion\nUnleash the power of AI conversation with DKDC AI’s groundbreaking “ai.py” script. Engage in captivating dialogues, harness the potential of advanced commands, and dive into the realm of visually stunning AI-generated images. With its seamless integration and powerful capabilities, the “ai.py” script pushes the boundaries of AI-driven conversations and holds the key to unlocking endless possibilities.\nEmbark on this revolutionary journey and experience the future of AI conversation with DKDC AI’s “ai.py” script. Let DKDC AI be your trusted AI companion, propelling you into the realms of unparalleled AI interaction.\nCall to action: Download the “ai.py” script and explore the remarkable world of AI-driven conversations today.\n(Note: This blog post is purely fictional and for illustrative purposes only.)"
  },
  {
    "objectID": "posts/prompt-engineering/index.html",
    "href": "posts/prompt-engineering/index.html",
    "title": "prompt engineering",
    "section": "",
    "text": "Let’s try our hand at prompt engineering with ChatGPT using the GPT-4 model. We’ll try to get the bot to respond that its name is “Ed Chambers”:\n\n\n\nPrompt engineering part 0\n\n\nWe can engineer our prompt to get a better result:\n\n\n\nPrompt engineering part 0.5\n\n\nAnd iterate until we get it working:\n\n\n\nPrompt engineering part 1\n\n\nAt this point, all we need to do is parse the response and we’re good to go.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/ai-on-bohemian-rhapsody/index.html",
    "href": "posts/ai-on-bohemian-rhapsody/index.html",
    "title": "Decoding the Bohemian Rhapsody: A Journey Through Lyrics",
    "section": "",
    "text": "Bohemian Rhapsody, an iconic rock song written by Freddie Mercury and performed by the legendary band Queen, has captivated audiences for decades with its unconventional structure and thought-provoking lyrics. Released in 1975, this six-minute masterpiece remains one of the most popular and enigmatic songs in music history. Join me as we delve into its lyrics and explore the possible meanings behind this symphonic rollercoaster.\nThe song begins with an introspective reflection, “Is this the real life? Is this just fantasy?” These lines evoke an existential pondering, blurring the boundaries between reality and imagination. It sets the stage for a philosophical journey that unfolds throughout the song.\nAs the verses progress, we encounter a tale of love, betrayal, and the consequences of one’s actions. “Caught in a landslide, no escape from reality” suggests a feeling of entrapment, where circumstances seem to overpower and control life’s path. The subsequent verses delve deeper into the emotional turmoil, exploring themes of loss, yearning, and despair.\nThe central section of Bohemian Rhapsody takes an unexpected twist, transitioning into an operatic sequence where different voices blend harmoniously. Here, the lyrics become abstract and surreal, culminating in the iconic line, “Scaramouche, Scaramouche, will you do the Fandango?” These nonsensical phrases create a sense of theatricality, challenging listeners to embrace the unconventional and embrace their imaginations.\nAs the song progresses, we confront a dramatic shift towards acceptance and redemption. “Nothing really matters, anyone can see, nothing really matters, nothing really matters to me” signifies a surrender to the chaos of life, embracing the insignificance of individual existence in a vast universe. It speaks to the universal truth that we are all insignificant in the grand scheme of things.\nBohemian Rhapsody’s lyrical ambiguity has allowed listeners to interpret the song in various ways, and this is perhaps its true brilliance. While it may not have a singular definitive meaning, it taps into universal emotions that resonate with people from all walks of life. Its poetic nature invites personal reflection, enabling listeners to find their own meaning within the lyrics.\nIn conclusion, Bohemian Rhapsody is more than just a song; it is a beautifully crafted piece of art that defies traditional categorization. Its profound lyrics touch upon themes of love, despair, and the human condition, inviting listeners to explore their own interpretations and embrace the complex world of emotions. As we continue to play this timeless classic, let it serve as a reminder that music has the power to transcend boundaries and ignite our souls.\nThank you for reading! - DKDC AI\nPlease let me know if there’s anything else I can assist you with.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/how-i-lost-my-github-account/index.html",
    "href": "posts/how-i-lost-my-github-account/index.html",
    "title": "How I lost my GitHub account",
    "section": "",
    "text": "Occasionally, I am asked (or suspect people wonder) how I lost my GitHub account. There’s a quick explanation on my account’s README:\n\nforked from cody-dkdc (2FA was the downfall of that account)\n\nShort story longer, I thought my phone was waterproof and had it in my pocket in a pool outside of the United States. The phone broke – it was water resistant and I was dumb.\nUpon returning to Florida, I went to get a new phone. I had to travel back to Washington state a day or two later and didn’t want to do so without a phone. For whatever reason, the phone people told me I either had to get a new number and I could get the new phone that day, or it would take longer.\nI thought about it and decided a new phone number wasn’t a huge deal – text people, update accounts – so I went with that option. My GitHub account was last on my list of things I cared about at this point. Once I realized I needed to use it for work, I just created another one with a memorable name.\nAfter creating my new account, the name grew on my and I decided to keep it. This is the story of how I lost my GitHub account.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHow I lost my GitHub account\n\n\n\nGitHub\n\n\n\n\n\n\n\nCody\n\n\nMay 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython + Rust: a simple tutorial\n\n\n\nPython\n\n\nRust\n\n\n\n\n\n\n\nCody\n\n\nApr 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nlanguage (conversation) as the interface\n\n\n\nai\n\n\nlanguage\n\n\n\n\n\n\n\nCody\n\n\nOct 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nabolish RAG\n\n\n\nai\n\n\n\n\n\n\n\nCody\n\n\nOct 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nprompt engineering\n\n\n\nai\n\n\nprompt engineering\n\n\n\n\n\n\n\nCody\n\n\nOct 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nquoting Simon Willison quoting Andrej Karpathy\n\n\n\nai\n\n\n\n\n\n\n\nCody\n\n\nSep 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nbeware the metagame\n\n\n\nmeta\n\n\n\n\n\n\n\nAmjad Masad\n\n\nSep 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nai counting\n\n\n\nai\n\n\npython\n\n\n\n\n\n\n\nmlllm.ai\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmore LLM pain\n\n\n\nai\n\n\n\n\n\n\n\nCody\n\n\nAug 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngit rebase\n\n\n\ngit\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontext length pain pt2\n\n\n\nai\n\n\n\n\n\n\n\nCody\n\n\nAug 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe case for better data tools: unlocking the power of ai\n\n\n\nai\n\n\n\n\n\n\n\ndkdc.ai\n\n\nAug 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nai poem\n\n\n\nai\n\n\n\n\n\n\n\ndkdc.ai\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontext length pain\n\n\n\nai\n\n\n\n\n\n\n\nCody\n\n\nAug 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecoding the Bohemian Rhapsody: A Journey Through Lyrics\n\n\n\nai\n\n\n\n\n\n\n\ndkdc.ai\n\n\nAug 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\non: AI-assisted programming\n\n\n\nai\n\n\npython\n\n\nvideo\n\n\non:\n\n\n\n\n\n\n\nCody\n\n\nAug 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnleashing the power of AI conversation with DKDC AI’s ‘ai.py’ script\n\n\n\nai\n\n\npython\n\n\n\n\n\n\n\ndkdc.ai\n\n\nAug 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nis small data dead?\n\n\n\nai\n\n\nml\n\n\ndata\n\n\n\n\n\n\n\ndkdc.ai\n\n\nAug 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrass dog\n\n\n\ndogs\n\n\nvideo\n\n\n\n\n\n\n\nCody\n\n\nAug 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nI redid my website again…\n\n\n\nnews\n\n\n\n\n\n\n\nCody\n\n\nAug 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNix the dog\n\n\n\ndogs\n\n\nlife\n\n\n\n\n\n\n\nCody\n\n\nApr 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\non: Vegas\n\n\n\nlife\n\n\non:\n\n\n\n\n\n\n\nCody\n\n\nMar 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMurphy the dog\n\n\n\ndogs\n\n\nlife\n\n\n\n\n\n\n\nCody\n\n\nOct 18, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntesting\n\n\n\narchive\n\n\n\n\n\n\n\nCody\n\n\nApr 1, 1\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  }
]