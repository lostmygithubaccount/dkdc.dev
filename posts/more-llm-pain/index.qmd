---
title: "more LLM pain"
author: "Cody"
date: "2023-08-18"
image: "thumbnail.png"
categories: 
    - ai
---

Argh:

> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | WARNING:langchain.llms.base:Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming
> | INFO:root:llming

Who do I have to pay and how much to get fast, high-throughput LLMs? And how much will you lose per API call?

