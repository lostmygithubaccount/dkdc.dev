---
title: "context length pain"
author: "Cody"
date: "2023-08-13"
image: "thumbnail.png"
categories: 
    - ai
---

The bane of my existence:

> InvalidRequestError: This model's maximum context length is 16385 tokens. However, your messages resulted in 28737 tokens. Please reduce the length of the messages.

Somebody please give me GPT4-32k access.

**Updates:**
See [part 2](/posts/context-length-pain-pt2).
